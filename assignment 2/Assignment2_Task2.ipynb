{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error\n",
    "from keras._tf_keras.keras.models import Sequential, load_model\n",
    "from keras._tf_keras.keras.layers import LSTM, Dense, Input\n",
    "import matplotlib.pyplot as plt\n",
    "from UsefulFunctions import create_sequences, rolling_forecast, FitLSTM, FitLSTM_1layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load electricity spot price data\n",
    "df = pd.read_csv(\"Elspotprices2nd.csv\")\n",
    "df[\"HourUTC\"] = pd.to_datetime(df[\"HourUTC\"])\n",
    "df.set_index(\"HourUTC\", inplace=True)\n",
    "df = df.sort_index()\n",
    "\n",
    "# Split data into training (Jan 2019 to Aug 2024) and testing (Sep 2024)\n",
    "lstm1_train_data = df.loc[:\"2024-08-31\"].values.reshape(-1, 1)\n",
    "lstm1_test_data = df.loc[\"2024-09-01\":\"2024-09-30\"].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize both training and testing data\n",
    "lstm1_scaler = MinMaxScaler()\n",
    "lstm1_train_scaled = lstm1_scaler.fit_transform(lstm1_train_data)\n",
    "lstm1_test_scaled = lstm1_scaler.transform(lstm1_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input features and hyperparameters\n",
    "window_size = 24\n",
    "n_lookahead = 24\n",
    "n_neurons = 64\n",
    "n_features = 1\n",
    "epochs = 10\n",
    "dropout = 0\n",
    "\n",
    "# Fit the LSTM model\n",
    "# The ordered_validation parameter is set to True to ensure that the validation set is ordered in time,\n",
    "# and not randomly shuffled. This is important for time series data to maintain the temporal order.\n",
    "lstm2_model = FitLSTM_1layer(lstm1_train_scaled, window_size, n_features, \n",
    "                             n_lookahead, n_neurons, epochs, dropout, ordered_validation=True)\n",
    "lstm2_model.save(\"lstm1_model_test.keras\")\n",
    "\n",
    "# Generate predictions\n",
    "lstm1_pred_scaled = rolling_forecast(lstm2_model, lstm1_train_scaled, lstm1_test_scaled, window_size, n_lookahead)\n",
    "\n",
    "# Inverse transform to get the actual values from the scaled predictions\n",
    "lstm1_pred_inv = lstm1_scaler.inverse_transform(lstm1_pred_scaled.reshape(-1, 1))\n",
    "\n",
    "# Forecast with persistence (naive forecast using previous values)\n",
    "Persistence = np.concatenate((lstm1_train_data[-n_lookahead:], # Start with last <n_lookahead> hours of training data\n",
    "                              lstm1_test_data[:-n_lookahead]), # Add the test data to the persistence forecast\n",
    "                              axis=0)  # Combine arrays\n",
    "\n",
    "# Calculate and print RMSE values for persistence and forecasts\n",
    "rmse_lstm1 = root_mean_squared_error(lstm1_test_data, lstm1_pred_inv)\n",
    "rmse_persistence = root_mean_squared_error(Persistence, lstm1_test_data)\n",
    "print(f\"RMSE for LSTM (No exogenous): {rmse_lstm1:.2f}\")\n",
    "print(f\"RMSE for Persistence: {rmse_persistence:.2f}\")\n",
    "\n",
    "# Plot the forecasts\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "plt.plot(np.arange(1, len(lstm1_train_data) + 1), lstm1_train_data, color=\"black\", label=\"Training set\")\n",
    "plt.plot(np.arange(len(lstm1_train_data) + 1, len(lstm1_train_data) + len(lstm1_pred_inv.flatten()) + 1), lstm1_pred_inv.flatten(), color=\"blue\", label=\"Forecasted values\")\n",
    "plt.plot(np.arange(len(lstm1_train_data) + 1, len(lstm1_train_data) + len(Persistence) + 1), Persistence, color=\"green\", label=\"Persistence\")\n",
    "plt.plot(np.arange(len(lstm1_train_data) + 1, len(lstm1_train_data) + len(lstm1_test_data.flatten()) + 1), lstm1_test_data.flatten(), color=\"red\", label=\"Actual values\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(alpha=0.25)\n",
    "plt.xlim([len(lstm1_train_data) - 7 * 24, len(lstm1_train_data) + len(lstm1_test_data)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version with multilayer LSTM\n",
    "The multilayer setup leads to higher rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input features and hyperparameters\n",
    "n_features = 1  # No external input features, only the target variable\n",
    "n_steps = 24  # The number of time steps the model will look back\n",
    "n_lookahead = 24  # The number of steps the model will predict ahead\n",
    "n_neurons = 64  # Number of neurons in the LSTM layer\n",
    "n_neurons_dense = 36  # Number of neurons in the dense layer\n",
    "epochs = 10  # Number of training epochs\n",
    "dropout1 = 0.05  # Dropout rate for the LSTM layer\n",
    "dropout2 = 0.05  # Dropout rate for the dense layer\n",
    "\n",
    "# Fit the LSTM model\n",
    "model = FitLSTM(lstm1_train_scaled, n_steps, n_features, n_lookahead, \n",
    "                n_neurons, n_neurons_dense, epochs, dropout1, dropout2)\n",
    "model.save(\"The_modelv2.keras\")\n",
    "\n",
    "# Generate predictions\n",
    "lstm1_pred_scaled = rolling_forecast(model, lstm1_train_scaled, lstm1_test_scaled, n_steps, n_lookahead)\n",
    "\n",
    "# Inverse transform to get the actual values from the scaled predictions\n",
    "Forecasts = lstm1_scaler.inverse_transform(lstm1_pred_scaled.reshape(-1, 1))\n",
    "\n",
    "# Forecast with persistence (naive forecast using previous values)\n",
    "Persistence = np.concatenate((lstm1_train_data[-n_lookahead:], # Start with last <n_lookahead> hours of training data\n",
    "                              lstm1_test_data[:-n_lookahead]), # Add the test data to the persistence forecast\n",
    "                              axis=0)  # Combine arrays\n",
    "\n",
    "# Calculate and print RMSE values for persistence and forecasts\n",
    "RMSE_P = root_mean_squared_error(Persistence, lstm1_test_data)\n",
    "RMSE_F = root_mean_squared_error(Forecasts, lstm1_test_data)\n",
    "print(\"RMSE for weekly persistence: \", round(RMSE_P))\n",
    "print(\"RMSE for forecasts: \", round(RMSE_F))\n",
    "\n",
    "# Plot the forecasts\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "plt.plot(np.arange(1, len(lstm1_train_data) + 1), lstm1_train_data, color=\"black\", label=\"Training set\")\n",
    "plt.plot(np.arange(len(lstm1_train_data) + 1, len(lstm1_train_data) + len(Forecasts) + 1), Forecasts, color=\"blue\", label=\"Forecasted values\")\n",
    "plt.plot(np.arange(len(lstm1_train_data) + 1, len(lstm1_train_data) + len(Persistence) + 1), Persistence, color=\"green\", label=\"Persistence\")\n",
    "plt.plot(np.arange(len(lstm1_train_data) + 1, len(lstm1_train_data) + len(lstm1_test_data) + 1), lstm1_test_data, color=\"red\", label=\"Actual values\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(alpha=0.25)\n",
    "plt.xlim([len(lstm1_train_data) - 7 * 24, len(lstm1_train_data) + len(lstm1_test_data)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load price data\n",
    "df_prices = pd.read_csv(\"Elspotprices2nd.csv\")\n",
    "df_prices[\"HourUTC\"] = pd.to_datetime(df_prices[\"HourUTC\"])\n",
    "df_prices.set_index(\"HourUTC\", inplace=True)\n",
    "df_prices = df_prices.sort_index()\n",
    "\n",
    "# Load exogenous data\n",
    "df_exo = pd.read_csv(\"ProdConData.csv\")\n",
    "df_exo[\"HourUTC\"] = pd.to_datetime(df_exo[\"HourUTC\"])\n",
    "df_exo.set_index(\"HourUTC\", inplace=True)\n",
    "df_exo = df_exo.sort_index()\n",
    "\n",
    "# Merge datasets\n",
    "df_combined = df_prices.join(df_exo, how='inner')\n",
    "\n",
    "# Select target + exogenous features\n",
    "exogenous_vars = [\"GrossConsumptionMWh\", \"OffshoreWindGe100MW_MWh\", \"SolarPowerGe40kW_MWh\"]\n",
    "lstm2_features = [\"SpotPriceDKK\"] + exogenous_vars\n",
    "df_lstm2 = df_combined[lstm2_features].dropna()\n",
    "\n",
    "# Train/test split\n",
    "lstm2_train = df_lstm2.loc[\"2019-01-01\":\"2024-08-31\"]\n",
    "lstm2_test = df_lstm2.loc[\"2024-09-01\":\"2024-09-30\"]\n",
    "\n",
    "# Normalize\n",
    "lstm2_scaler = MinMaxScaler()\n",
    "lstm2_train_scaled = lstm2_scaler.fit_transform(lstm2_train)\n",
    "lstm2_test_scaled = lstm2_scaler.transform(lstm2_test)\n",
    "\n",
    "# Create multivariate sequences\n",
    "def create_sequences_multivariate(data, window_size=24):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - 23):\n",
    "        X.append(data[i:i+window_size])              # Input: 24x4\n",
    "        y.append(data[i+window_size:i+window_size+24, 0])  # Output: 24 prices\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "lstm2_window_size = 24\n",
    "lstm2_X_train, lstm2_y_train = create_sequences_multivariate(lstm2_train_scaled, lstm2_window_size)\n",
    "\n",
    "# Build model\n",
    "lstm2_model = Sequential()\n",
    "lstm2_model.add(LSTM(64, input_shape=(lstm2_window_size, len(lstm2_features))))\n",
    "lstm2_model.add(Dense(24))  # Predict 24 hours\n",
    "lstm2_model.compile(loss='mse', optimizer='adam')\n",
    "lstm2_model.fit(lstm2_X_train, lstm2_y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Forecast function\n",
    "def rolling_forecast_multivariate(model, test_scaled, window_size=24, n_features=4):\n",
    "    predictions = []\n",
    "    for day in range(30):\n",
    "        start_idx = day * 24\n",
    "        input_seq = test_scaled[start_idx:start_idx+window_size]\n",
    "        input_seq = input_seq.reshape(1, window_size, n_features)\n",
    "        pred_scaled = model.predict(input_seq, verbose=0)[0]\n",
    "        predictions.append(pred_scaled)\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Forecast\n",
    "lstm2_pred_scaled = rolling_forecast_multivariate(\n",
    "    lstm2_model, lstm2_test_scaled, window_size=lstm2_window_size, n_features=len(lstm2_features)\n",
    ")\n",
    "\n",
    "# Inverse transform only price predictions\n",
    "lstm2_pred_flat = lstm2_pred_scaled.reshape(-1, 1)\n",
    "dummy = np.zeros((lstm2_pred_flat.shape[0], len(lstm2_features)))\n",
    "dummy[:, 0] = lstm2_pred_flat[:, 0]\n",
    "lstm2_pred_inv = lstm2_scaler.inverse_transform(dummy)[:, 0].reshape(30, 24)\n",
    "\n",
    "# Ground truth\n",
    "lstm2_true = lstm2_test[\"SpotPriceDKK\"].values.reshape(30, 24)\n",
    "\n",
    "# Compute RMSE\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "rmse_lstm2 = compute_rmse(lstm2_true, lstm2_pred_inv)\n",
    "print(f\"LSTM (with exogenous vars) RMSE: {rmse_lstm2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task 2 – Long‑Short‑Term‑Memory (LSTM) day‑ahead forecaster\n",
    "WITH up to three exogenous variables\n",
    "——————————————————————————————————————————————\n",
    "• Target          : DK2 spot price  (column “SpotPriceDKK”)\n",
    "• Exogenous input : GrossConsumptionMWh,\n",
    "                    OffshoreWindGe100MW_MWh,\n",
    "                    SolarPowerGe40kW_MWh\n",
    "• Forecast horizon: 24 h  (n_lookahead)\n",
    "• Look‑back window: 48 h  (n_steps)          ← identical to the improved univariate script\n",
    "• Split           : 2019‑01‑01 – 2024‑08‑31 → train\n",
    "                    2024‑09‑01 – 2024‑09‑30 → test\n",
    "• Model           : 2‑layer LSTM from UsefulFunctions.LSTM_multilayer\n",
    "• Evaluation      : RMSE of LSTM forecast vs. weekly‑persistence benchmark\n",
    "\"\"\"\n",
    "\n",
    "# ───────────────────────── Imports ──────────────────────────\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential  # only needed if you want to tweak the model further\n",
    "\n",
    "from UsefulFunctions import LSTM_multilayer        # multi‑layer LSTM builder\n",
    "# (rolling_forecast from UsefulFunctions is 1‑D; we implement a multivariate analogue below)\n",
    "\n",
    "# ───────────────────── Helper functions ─────────────────────\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"Wrapper around sklearn’s MSE that returns the square root (RMSE).\"\"\"\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "def create_sequences_multivariate(data: np.ndarray,\n",
    "                                  window_size: int,\n",
    "                                  n_lookahead: int):\n",
    "    \"\"\"\n",
    "    Build (X, y) pairs for a *multivariate* sequence‑to‑sequence forecast.\n",
    "      X shape → [samples, window_size, n_features]\n",
    "      y shape → [samples, n_lookahead]    (only the *price* column is predicted)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - n_lookahead + 1):\n",
    "        X.append(data[i:i + window_size, :])                 # last 48 h, all features\n",
    "        y.append(data[i + window_size:i + window_size + n_lookahead, 0])  # next 24 prices\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def FitLSTM_multivariate(train_scaled: np.ndarray,\n",
    "                         n_steps: int,\n",
    "                         n_features: int,\n",
    "                         n_lookahead: int,\n",
    "                         n_neurons: int,\n",
    "                         n_neurons_dense: int,\n",
    "                         epochs: int,\n",
    "                         dropout1: float,\n",
    "                         dropout2: float):\n",
    "    \"\"\"\n",
    "    Thin wrapper that (i) turns the multivariate windowed data into tensors,\n",
    "    (ii) calls UsefulFunctions.LSTM_multilayer, (iii) trains the network,\n",
    "    (iv) plots learning curves, and (v) returns the fitted model.\n",
    "    \"\"\"\n",
    "    # --- 1 ▸ build training tensors\n",
    "    X_train, y_train = create_sequences_multivariate(\n",
    "        train_scaled, n_steps, n_lookahead\n",
    "    )\n",
    "\n",
    "    # --- 2 ▸ instantiate the model\n",
    "    model = LSTM_multilayer(\n",
    "        n_steps,            # same interface as in UsefulFunctions.FitLSTM\n",
    "        n_features,\n",
    "        n_neurons,\n",
    "        n_neurons_dense,\n",
    "        n_lookahead,\n",
    "        dropout1,\n",
    "        dropout2,\n",
    "    )\n",
    "\n",
    "    # --- 3 ▸ train\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # --- 4 ▸ diagnostics\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history.history[\"loss\"], label=\"Training loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.title(\"Learning‑curve (multivariate LSTM)\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def rolling_forecast_multivariate(model,\n",
    "                                  history_scaled: np.ndarray,\n",
    "                                  test_scaled: np.ndarray,\n",
    "                                  window_size: int,\n",
    "                                  n_lookahead: int):\n",
    "    \"\"\"\n",
    "    Multivariate, *rolling* day‑ahead forecast identical in spirit to\n",
    "    UsefulFunctions.rolling_forecast but supporting n_features > 1.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    data = np.concatenate((history_scaled, test_scaled), axis=0)\n",
    "    n_train = len(history_scaled)\n",
    "    n_test  = len(test_scaled)\n",
    "    N_days  = int(n_test / n_lookahead)                      # == 30 for September\n",
    "\n",
    "    for day in range(N_days):\n",
    "        start = n_train + day * n_lookahead - window_size\n",
    "        end   = n_train + day * n_lookahead\n",
    "        input_seq = data[start:end].reshape((1, window_size, data.shape[1]))\n",
    "\n",
    "        pred_scaled = model.predict(input_seq, verbose=0)[0]  # shape (24,)\n",
    "        predictions.append(pred_scaled)\n",
    "\n",
    "    return np.array(predictions)                              # (30, 24)\n",
    "\n",
    "\n",
    "# ─────────────────────── Data ingestion ─────────────────────\n",
    "# ➊ price\n",
    "df_p = pd.read_csv(\"Elspotprices2nd.csv\")\n",
    "df_p[\"HourUTC\"] = pd.to_datetime(df_p[\"HourUTC\"])\n",
    "df_p.set_index(\"HourUTC\", inplace=True)\n",
    "df_p.sort_index(inplace=True)\n",
    "\n",
    "# ➋ exogenous variables\n",
    "exo_vars = [\"GrossConsumptionMWh\",\n",
    "            \"OffshoreWindGe100MW_MWh\",\n",
    "            \"SolarPowerGe40kW_MWh\"]\n",
    "\n",
    "df_x = pd.read_csv(\"ProdConData.csv\")\n",
    "df_x[\"HourUTC\"] = pd.to_datetime(df_x[\"HourUTC\"])\n",
    "df_x.set_index(\"HourUTC\", inplace=True)\n",
    "df_x.sort_index(inplace=True)\n",
    "\n",
    "# ➌ merge – keep only rows where every column is present\n",
    "df = df_p.join(df_x[exo_vars], how=\"inner\")\\\n",
    "         .rename(columns={\"SpotPriceDKK\": \"Price\"})\n",
    "\n",
    "# ─────────────── Train–test split & scaling ────────────────\n",
    "train_df = df.loc[:\"2024-08-31\"]\n",
    "test_df  = df.loc[\"2024-09-01\":\"2024-09-30\"]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_df)\n",
    "test_scaled  = scaler.transform(test_df)\n",
    "\n",
    "# ───────────────────── Hyper‑parameters ─────────────────────\n",
    "n_features       = train_scaled.shape[1]      # = 4  (price + 3 exo)\n",
    "n_steps          = 48\n",
    "n_lookahead      = 24\n",
    "n_neurons        = 150\n",
    "n_neurons_dense  = 60\n",
    "epochs           = 25\n",
    "dropout1         = 0.10\n",
    "dropout2         = 0.10\n",
    "\n",
    "# ─────────────────────── Model training ─────────────────────\n",
    "model = FitLSTM_multivariate(\n",
    "    train_scaled,\n",
    "    n_steps, n_features, n_lookahead,\n",
    "    n_neurons, n_neurons_dense,\n",
    "    epochs, dropout1, dropout2\n",
    ")\n",
    "model.save(\"LSTM_Task2_withExo_v2.keras\")\n",
    "\n",
    "# ───────────────────────── Inference ────────────────────────\n",
    "pred_scaled = rolling_forecast_multivariate(\n",
    "    model, train_scaled, test_scaled,\n",
    "    n_steps, n_lookahead\n",
    ")                                           # shape (30, 24)\n",
    "\n",
    "# — inverse‑transform ONLY the price column —\n",
    "flat_pred = pred_scaled.flatten()[:, None]                # (720, 1)\n",
    "dummy     = np.zeros((flat_pred.shape[0], n_features))\n",
    "dummy[:, 0] = flat_pred[:, 0]                             # price → first column\n",
    "price_forecast = scaler.inverse_transform(dummy)[:, 0]\\\n",
    "                     .reshape(pred_scaled.shape)          # (30, 24)\n",
    "\n",
    "# ─────────────────────── Benchmarks ─────────────────────────\n",
    "# Weekly persistence: last 24 h of TRAIN + actuals up to t‑24\n",
    "persistence = np.concatenate(\n",
    "    (train_df[\"Price\"].values[-n_lookahead:],\n",
    "     test_df[\"Price\"].values[:-n_lookahead])\n",
    ")\n",
    "\n",
    "# For RMSE we need identical shapes\n",
    "rmse_persistence = root_mean_squared_error(\n",
    "    test_df[\"Price\"].values, persistence\n",
    ")\n",
    "\n",
    "rmse_forecast = root_mean_squared_error(\n",
    "    test_df[\"Price\"].values, price_forecast.flatten()\n",
    ")\n",
    "\n",
    "# ────────────────────────── Plots ───────────────────────────\n",
    "plt.figure(figsize=(10, 4), dpi=110)\n",
    "t_train = np.arange(len(train_df))\n",
    "t_test  = np.arange(len(train_df), len(train_df) + len(test_df))\n",
    "\n",
    "plt.plot(t_train, train_df[\"Price\"].values,  color=\"black\", label=\"Training set\")\n",
    "plt.plot(t_test,  test_df[\"Price\"].values,   color=\"red\",   label=\"Actual price (test)\")\n",
    "plt.plot(t_test,  price_forecast.flatten(), color=\"blue\",  label=\"LSTM forecast\")\n",
    "# plt.plot(t_test, persistence,              color=\"green\", label=\"Persistence\")\n",
    "\n",
    "plt.xlim([len(train_df) - 7*24, len(train_df) + len(test_df)])\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ──────────────────────── Results ───────────────────────────\n",
    "print(f\"RMSE – weekly persistence : {rmse_persistence:,.0f} DKK/MWh\")\n",
    "print(f\"RMSE – LSTM with exo vars : {rmse_forecast:,.0f} DKK/MWh\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnergyAnalyticsF25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
