{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from UsefulFunctions import rolling_forecast, rolling_forecast_multivariate, FitLSTM_multilayer, FitLSTM_1layer, FitLSTM_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load electricity spot price data\n",
    "df = pd.read_csv(\"Elspotprices2nd.csv\")\n",
    "df[\"HourUTC\"] = pd.to_datetime(df[\"HourUTC\"])\n",
    "df.set_index(\"HourUTC\", inplace=True)\n",
    "df = df.sort_index()\n",
    "\n",
    "# Split data into training (Jan 2019 to Aug 2024) and testing (Sep 2024)\n",
    "lstm1_train_data = df.loc[:\"2024-08-31\"].values.reshape(-1, 1)\n",
    "lstm1_test_data = df.loc[\"2024-09-01\":\"2024-09-30\"].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize both training and testing data\n",
    "lstm1_scaler = MinMaxScaler()\n",
    "lstm1_train_scaled = lstm1_scaler.fit_transform(lstm1_train_data)\n",
    "lstm1_test_scaled = lstm1_scaler.transform(lstm1_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input features and hyperparameters\n",
    "window_size = 24\n",
    "n_lookahead = 24\n",
    "n_neurons = 64\n",
    "n_features = 1\n",
    "epochs = 10\n",
    "dropout = 0\n",
    "\n",
    "# Fit the LSTM model\n",
    "# The ordered_validation parameter is set to True to ensure that the validation set is ordered in time,\n",
    "# and not randomly shuffled. This is important for time series data to maintain the temporal order.\n",
    "lstm2_model = FitLSTM_1layer(lstm1_train_scaled, window_size, n_features, \n",
    "                             n_lookahead, n_neurons, epochs, dropout, ordered_validation=True)\n",
    "# lstm2_model.save(\"lstm1_model_test.keras\") # Uncomment to save the model\n",
    "\n",
    "# Generate predictions\n",
    "lstm1_pred_scaled = rolling_forecast(lstm2_model, lstm1_train_scaled, lstm1_test_scaled, window_size, n_lookahead)\n",
    "\n",
    "# Inverse transform to get the actual values from the scaled predictions\n",
    "lstm1_pred_inv = lstm1_scaler.inverse_transform(lstm1_pred_scaled.reshape(-1, 1))\n",
    "\n",
    "# Forecast with persistence (naive forecast using previous values)\n",
    "Persistence = np.concatenate((lstm1_train_data[-n_lookahead:], # Start with last <n_lookahead> hours of training data\n",
    "                              lstm1_test_data[:-n_lookahead]), # Add the test data to the persistence forecast\n",
    "                              axis=0)  # Combine arrays\n",
    "\n",
    "# Calculate and print RMSE values for persistence and forecasts\n",
    "rmse_lstm1 = root_mean_squared_error(lstm1_test_data, lstm1_pred_inv)\n",
    "rmse_persistence = root_mean_squared_error(Persistence, lstm1_test_data)\n",
    "print(f\"RMSE for LSTM (No exogenous): {rmse_lstm1:.2f}\")\n",
    "print(f\"RMSE for Persistence: {rmse_persistence:.2f}\")\n",
    "\n",
    "# Plot the forecasts\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "plt.plot(np.arange(1, len(lstm1_train_data) + 1), lstm1_train_data, color=\"black\", label=\"Training set\")\n",
    "plt.plot(np.arange(len(lstm1_train_data) + 1, len(lstm1_train_data) + len(lstm1_pred_inv.flatten()) + 1), lstm1_pred_inv.flatten(), color=\"blue\", label=\"Forecasted values\")\n",
    "plt.plot(np.arange(len(lstm1_train_data) + 1, len(lstm1_train_data) + len(Persistence) + 1), Persistence, color=\"green\", label=\"Persistence\")\n",
    "plt.plot(np.arange(len(lstm1_train_data) + 1, len(lstm1_train_data) + len(lstm1_test_data.flatten()) + 1), lstm1_test_data.flatten(), color=\"red\", label=\"Actual values\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(alpha=0.25)\n",
    "plt.xlim([len(lstm1_train_data) - 7 * 24, len(lstm1_train_data) + len(lstm1_test_data)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version with multilayer LSTM\n",
    "The multilayer setup leads to higher rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input features and hyperparameters\n",
    "n_features = 1  # No external input features, only the target variable\n",
    "n_steps = 24  # The number of time steps the model will look back\n",
    "n_lookahead = 24  # The number of steps the model will predict ahead\n",
    "n_neurons = 64  # Number of neurons in the LSTM layer\n",
    "n_neurons_dense = 36  # Number of neurons in the dense layer\n",
    "epochs = 10  # Number of training epochs\n",
    "dropout1 = 0.05  # Dropout rate for the LSTM layer\n",
    "dropout2 = 0.05  # Dropout rate for the dense layer\n",
    "\n",
    "# Fit the LSTM model\n",
    "model = FitLSTM_multilayer(lstm1_train_scaled, n_steps, n_features, n_lookahead, \n",
    "                n_neurons, n_neurons_dense, epochs, dropout1, dropout2)\n",
    "# model.save(\"The_modelv2.keras\") # Uncomment to save the model\n",
    "\n",
    "# Generate predictions\n",
    "lstm1_pred_scaled = rolling_forecast(model, lstm1_train_scaled, lstm1_test_scaled, n_steps, n_lookahead)\n",
    "\n",
    "# Inverse transform to get the actual values from the scaled predictions\n",
    "Forecasts = lstm1_scaler.inverse_transform(lstm1_pred_scaled.reshape(-1, 1))\n",
    "\n",
    "# Forecast with persistence (naive forecast using previous values)\n",
    "Persistence = np.concatenate((lstm1_train_data[-n_lookahead:], # Start with last <n_lookahead> hours of training data\n",
    "                              lstm1_test_data[:-n_lookahead]), # Add the test data to the persistence forecast\n",
    "                              axis=0)  # Combine arrays\n",
    "\n",
    "# Calculate and print RMSE values for persistence and forecasts\n",
    "RMSE_P = root_mean_squared_error(Persistence, lstm1_test_data)\n",
    "RMSE_F = root_mean_squared_error(Forecasts, lstm1_test_data)\n",
    "print(\"RMSE for weekly persistence: \", round(RMSE_P))\n",
    "print(\"RMSE for forecasts: \", round(RMSE_F))\n",
    "\n",
    "# Plot the forecasts\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "plt.plot(np.arange(1, len(lstm1_train_data) + 1), lstm1_train_data, color=\"black\", label=\"Training set\")\n",
    "plt.plot(np.arange(len(lstm1_train_data) + 1, len(lstm1_train_data) + len(Forecasts) + 1), Forecasts, color=\"blue\", label=\"Forecasted values\")\n",
    "plt.plot(np.arange(len(lstm1_train_data) + 1, len(lstm1_train_data) + len(Persistence) + 1), Persistence, color=\"green\", label=\"Persistence\")\n",
    "plt.plot(np.arange(len(lstm1_train_data) + 1, len(lstm1_train_data) + len(lstm1_test_data) + 1), lstm1_test_data, color=\"red\", label=\"Actual values\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(alpha=0.25)\n",
    "plt.xlim([len(lstm1_train_data) - 7 * 24, len(lstm1_train_data) + len(lstm1_test_data)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load price data\n",
    "df_prices = pd.read_csv(\"Elspotprices2nd.csv\")\n",
    "df_prices[\"HourUTC\"] = pd.to_datetime(df_prices[\"HourUTC\"])\n",
    "df_prices.set_index(\"HourUTC\", inplace=True)\n",
    "df_prices = df_prices.sort_index()\n",
    "\n",
    "# Load exogenous data\n",
    "df_exo = pd.read_csv(\"ProdConData.csv\")\n",
    "df_exo[\"HourUTC\"] = pd.to_datetime(df_exo[\"HourUTC\"])\n",
    "df_exo.set_index(\"HourUTC\", inplace=True)\n",
    "df_exo = df_exo.sort_index()\n",
    "\n",
    "# Merge datasets\n",
    "df_combined = df_prices.join(df_exo, how='inner')\n",
    "\n",
    "# Select target + exogenous features\n",
    "exogenous_vars = [\"GrossConsumptionMWh\", \"OffshoreWindGe100MW_MWh\", \"SolarPowerGe40kW_MWh\"]\n",
    "lstm2_features = [\"SpotPriceDKK\"] + exogenous_vars\n",
    "df_lstm2 = df_combined[lstm2_features].dropna()\n",
    "\n",
    "# Train/test split\n",
    "lstm2_train = df_lstm2.loc[:\"2024-08-31\"]\n",
    "lstm2_test = df_lstm2.loc[\"2024-09-01\":\"2024-09-30\"]\n",
    "\n",
    "# Normalize\n",
    "lstm2_scaler = MinMaxScaler()\n",
    "lstm2_train_scaled = lstm2_scaler.fit_transform(lstm2_train)\n",
    "lstm2_test_scaled = lstm2_scaler.transform(lstm2_test)\n",
    "\n",
    "# Fit a separate scaler for the target variable to inverse transform the forecasted values back to original scale\n",
    "# This ensures that the predictions we make can be transformed back to the original scale\n",
    "lstm2_scaler_y = MinMaxScaler()\n",
    "lstm2_scaler_y = lstm2_scaler_y.fit(lstm2_train[\"SpotPriceDKK\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input features and hyperparameters\n",
    "window_size = 24\n",
    "n_lookahead = 24\n",
    "n_neurons = 80\n",
    "n_features = len(lstm2_features)\n",
    "epochs = 10\n",
    "dropout = 0.06\n",
    "\n",
    "lstm2_model = FitLSTM_ext(lstm2_train_scaled, window_size, n_features, n_lookahead, \n",
    "                          n_neurons, epochs, dropout, ordered_validation=True)\n",
    "\n",
    "# Forecast\n",
    "lstm2_pred_scaled = rolling_forecast_multivariate(lstm2_model, lstm2_train_scaled, \n",
    "                          lstm2_test_scaled, window_size, n_lookahead, n_features)\n",
    "\n",
    "# Inverse transform only price predictions\n",
    "lstm2_pred_inv = lstm2_scaler_y.inverse_transform(lstm2_pred_scaled.reshape(-1, 1))[:,0]\n",
    "\n",
    "# Ground truth\n",
    "lstm2_true = lstm2_test[\"SpotPriceDKK\"].values\n",
    "\n",
    "# RMSE for the forecasted values\n",
    "RMSE_F = root_mean_squared_error(lstm2_true, lstm2_pred_inv)  \n",
    "print(f\"LSTM (with exogenous vars) RMSE: {RMSE_F:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm2_train_true = lstm2_train[\"SpotPriceDKK\"].values\n",
    "# Plot the forecasts\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "plt.plot(np.arange(1, len(lstm2_train_true) + 1), lstm2_train_true, color=\"black\", label=\"Training set\")\n",
    "plt.plot(np.arange(len(lstm2_train_true) + 1, len(lstm2_train_true) + len(lstm2_pred_inv.flatten()) + 1), lstm2_pred_inv.flatten(), color=\"blue\", label=\"Forecasted values\")\n",
    "plt.plot(np.arange(len(lstm2_train_true) + 1, len(lstm2_train_true) + len(lstm2_true.flatten()) + 1), lstm2_true.flatten(), color=\"red\", label=\"Actual values\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(alpha=0.25)\n",
    "plt.xlim([len(lstm2_train_true) - 7 * 24, len(lstm2_train_true) + len(lstm2_true)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm2_model.save(\"lstm2_model_good3.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnergyAnalyticsF25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
